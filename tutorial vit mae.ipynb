{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "123e4d01-2723-41c4-b2c2-bbb00edc1651",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pretraining ViT-MAE on GOES Satellite Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a242388e-dab5-41cb-a8ae-d2d9eaa5e7d2",
   "metadata": {},
   "source": [
    "In this tutorial, we’ll guide you through the process of pretraining a masked autoencoder (MAE) built on a Vision Transformer (ViT) using 10 weeks of GOES-16 satellite data. Before diving into supervised learning tasks in climate and Earth science, it’s critical to first build a model that truly understands the structure of the data itself even without labels. Satellite imagery, especially from instruments like GOES-16, captures complex, high-dimensional patterns in the atmosphere, including cloud motion, temperature gradients, and convective activity, but making sense of these patterns requires a model that can learn these spatial representations. \n",
    "\n",
    "In this context, our goal is to pretrain a Vision Transformer (ViT) using a Masked Autoencoder (MAE) approach. By hiding random patches of satellite data and asking the model to reconstruct what’s missing, we force it to learn underlying spatial patterns. This self-supervised approach is particularly useful when extracting meaningful visual representations from Earth observation data, even when labeled datasets are limited. With a pretrained model, we can later fine-tune on more specific downstream tasks, such as detecting the time of day, predicting extreme weather events, classifying cloud types, and more. \n",
    "\n",
    "To begin, we'll start off by installing and importing the essential frameworks, including the libraries mentioned prior, the ViT-MAE Hugging Face transformer, and core PyTorch modules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e67dfd9-b0ef-45e6-bf3a-4affac598dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numcodecs==0.15.1 in /opt/conda/envs/marble/lib/python3.11/site-packages (from numcodecs[pcodec]==0.15.1) (0.15.1)\n",
      "Requirement already satisfied: xarray==2025.3.1 in /opt/conda/envs/marble/lib/python3.11/site-packages (2025.3.1)\n",
      "Requirement already satisfied: zarr==3.0.6 in /opt/conda/envs/marble/lib/python3.11/site-packages (3.0.6)\n",
      "Requirement already satisfied: numpy>=1.24 in /opt/conda/envs/marble/lib/python3.11/site-packages (from numcodecs==0.15.1->numcodecs[pcodec]==0.15.1) (1.26.4)\n",
      "Requirement already satisfied: deprecated in /opt/conda/envs/marble/lib/python3.11/site-packages (from numcodecs==0.15.1->numcodecs[pcodec]==0.15.1) (1.2.18)\n",
      "Requirement already satisfied: packaging>=23.2 in /opt/conda/envs/marble/lib/python3.11/site-packages (from xarray==2025.3.1) (24.1)\n",
      "Requirement already satisfied: pandas>=2.1 in /opt/conda/envs/marble/lib/python3.11/site-packages (from xarray==2025.3.1) (2.2.2)\n",
      "Requirement already satisfied: donfig>=0.8 in /opt/conda/envs/marble/lib/python3.11/site-packages (from zarr==3.0.6) (0.8.1.post1)\n",
      "Requirement already satisfied: typing-extensions>=4.9 in /opt/conda/envs/marble/lib/python3.11/site-packages (from zarr==3.0.6) (4.12.2)\n",
      "Requirement already satisfied: pcodec<0.4,>=0.3 in /opt/conda/envs/marble/lib/python3.11/site-packages (from numcodecs[pcodec]==0.15.1) (0.3.5)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/marble/lib/python3.11/site-packages (from donfig>=0.8->zarr==3.0.6) (6.0.1)\n",
      "Requirement already satisfied: crc32c>=2.7 in /opt/conda/envs/marble/lib/python3.11/site-packages (from numcodecs[crc32c]>=0.14->zarr==3.0.6) (2.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/marble/lib/python3.11/site-packages (from pandas>=2.1->xarray==2025.3.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/marble/lib/python3.11/site-packages (from pandas>=2.1->xarray==2025.3.1) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/envs/marble/lib/python3.11/site-packages (from pandas>=2.1->xarray==2025.3.1) (2024.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/envs/marble/lib/python3.11/site-packages (from deprecated->numcodecs==0.15.1->numcodecs[pcodec]==0.15.1) (1.16.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/marble/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=2.1->xarray==2025.3.1) (1.16.0)\n",
      "Requirement already satisfied: transformers==4.32 in /opt/conda/envs/marble/lib/python3.11/site-packages (4.32.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/envs/marble/lib/python3.11/site-packages (4.0.0)\n",
      "Requirement already satisfied: torch in /opt/conda/envs/marble/lib/python3.11/site-packages (2.7.1)\n",
      "Requirement already satisfied: accelerate in /opt/conda/envs/marble/lib/python3.11/site-packages (1.9.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/marble/lib/python3.11/site-packages (from transformers==4.32) (3.15.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/envs/marble/lib/python3.11/site-packages (from transformers==4.32) (0.34.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/marble/lib/python3.11/site-packages (from transformers==4.32) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/marble/lib/python3.11/site-packages (from transformers==4.32) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/marble/lib/python3.11/site-packages (from transformers==4.32) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/marble/lib/python3.11/site-packages (from transformers==4.32) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/marble/lib/python3.11/site-packages (from transformers==4.32) (2.32.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/envs/marble/lib/python3.11/site-packages (from transformers==4.32) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/envs/marble/lib/python3.11/site-packages (from transformers==4.32) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/marble/lib/python3.11/site-packages (from transformers==4.32) (4.66.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/envs/marble/lib/python3.11/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/envs/marble/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/marble/lib/python3.11/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /opt/conda/envs/marble/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/envs/marble/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/conda/envs/marble/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/envs/marble/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/envs/marble/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/marble/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/marble/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /opt/conda/envs/marble/lib/python3.11/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /opt/conda/envs/marble/lib/python3.11/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /opt/conda/envs/marble/lib/python3.11/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /opt/conda/envs/marble/lib/python3.11/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /opt/conda/envs/marble/lib/python3.11/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /opt/conda/envs/marble/lib/python3.11/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /opt/conda/envs/marble/lib/python3.11/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /opt/conda/envs/marble/lib/python3.11/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /opt/conda/envs/marble/lib/python3.11/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /opt/conda/envs/marble/lib/python3.11/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /opt/conda/envs/marble/lib/python3.11/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /opt/conda/envs/marble/lib/python3.11/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /opt/conda/envs/marble/lib/python3.11/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /opt/conda/envs/marble/lib/python3.11/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /opt/conda/envs/marble/lib/python3.11/site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/conda/envs/marble/lib/python3.11/site-packages (from triton==3.3.1->torch) (70.1.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/marble/lib/python3.11/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/envs/marble/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.9.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/envs/marble/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.32) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/marble/lib/python3.11/site-packages (from requests->transformers==4.32) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/marble/lib/python3.11/site-packages (from requests->transformers==4.32) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/marble/lib/python3.11/site-packages (from requests->transformers==4.32) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/marble/lib/python3.11/site-packages (from requests->transformers==4.32) (2024.6.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/envs/marble/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/marble/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/marble/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/marble/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/envs/marble/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/marble/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/marble/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/marble/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/marble/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/marble/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.9.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/marble/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/marble/lib/python3.11/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/opt/conda/envs/marble/lib/python3.11/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# installing additional libraries into kernel \n",
    "import sys\n",
    "import subprocess\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"numcodecs[pcodec]==0.15.1\", \"xarray==2025.3.1\", \"zarr==3.0.6\"])\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"transformers==4.32\", \"datasets\", \"torch\", \"accelerate\"])\n",
    "\n",
    "# scientific computing\n",
    "import random\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch and ViT transformer\n",
    "import os\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import ViTImageProcessor, ViTMAEForPreTraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78da88f0-860f-4bfc-8cde-39bf5db3f397",
   "metadata": {},
   "source": [
    "Along with the previously mentioned libraries, including `numpy` and `xarray`, we are installing and importing Hugging Face's ViT-MAE transformer and PyTorch. We will leverage the ViT-MAE model, a prebuilt architecture designed specifically for masked autoencoding, to bypass the need to manually construct ViT layers or implement custom masking logic. PyTorch allows us to train the model as a flexible and widely-used deep learning framework that supports efficient optimization and backpropagation throughout the training loop. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c4fad3-bedd-4a2b-a9ae-a315a72995f4",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9867f727-392b-4fe2-8ba0-750e3a5eec9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set configs\n",
    "zarr_path   = \"/notebook_dir/public/mickellals-public/goes-16-2003-10-weeks.tmp.zarr\"\n",
    "channel    = \"CMI_C13\" \n",
    "lat_range   = None \n",
    "lon_range   = None \n",
    "crop_size     = 224 \n",
    "mask_ratio    = 0.5\n",
    "learning_rate = 5e-4 \n",
    "num_epochs    = 100\n",
    "checkpoint = 20 \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c472f438-2972-4849-8c9e-f22465c7d0b7",
   "metadata": {},
   "source": [
    "Here, we define the key configuration parameters that will control our training process. These parameters allow us to flexibly control dataset loading, training performance, and resource usage.\n",
    "\n",
    "`zarr_path`: Path to the GOES-16 satellite imagery dataset stored in zarr format.\n",
    "\n",
    "`channel`: The selected band (clean longwave window aka band #13 is selected in this case).\n",
    "\n",
    "`lat_range`, `lon_range`: Tuple formatting for latitude/longitude bounds to spatially crop the dataset. Set to `None` to disable.\n",
    "\n",
    "`crop_size`: Size (in pixels) of the square image patches that will be fed into the Vision Transformer.\n",
    "\n",
    "`mask_ratio`: Proportion of image patches to randomly mask during MAE pretraining.\n",
    "\n",
    "`learning_rate`: Initial learning rate for the AdamW optimizer.  \n",
    "\n",
    "`num_epochs`: Total number of training epochs.  \n",
    "\n",
    "`checkpoint`: Interval (in epochs) at which we save model checkpoints and plot training loss.\n",
    "\n",
    "`device`: Whether to use GPU (\"cuda\") or CPU, depending on system availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6601ff6-1833-4cd8-89dd-abb47ba298d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/marble/lib/python3.11/site-packages/numcodecs/zarr3.py:170: UserWarning: Numcodecs codecs are not in the Zarr version 3 specification and may not be supported by other zarr implementations.\n",
      "  super().__init__(**codec_config)\n",
      "/opt/conda/envs/marble/lib/python3.11/site-packages/numcodecs/zarr3.py:170: UserWarning: Numcodecs codecs are not in the Zarr version 3 specification and may not be supported by other zarr implementations.\n",
      "  super().__init__(**codec_config)\n",
      "/opt/conda/envs/marble/lib/python3.11/site-packages/numcodecs/zarr3.py:170: UserWarning: Numcodecs codecs are not in the Zarr version 3 specification and may not be supported by other zarr implementations.\n",
      "  super().__init__(**codec_config)\n",
      "/opt/conda/envs/marble/lib/python3.11/site-packages/numcodecs/zarr3.py:170: UserWarning: Numcodecs codecs are not in the Zarr version 3 specification and may not be supported by other zarr implementations.\n",
      "  super().__init__(**codec_config)\n",
      "/opt/conda/envs/marble/lib/python3.11/site-packages/numcodecs/zarr3.py:170: UserWarning: Numcodecs codecs are not in the Zarr version 3 specification and may not be supported by other zarr implementations.\n",
      "  super().__init__(**codec_config)\n",
      "/opt/conda/envs/marble/lib/python3.11/site-packages/numcodecs/zarr3.py:170: UserWarning: Numcodecs codecs are not in the Zarr version 3 specification and may not be supported by other zarr implementations.\n",
      "  super().__init__(**codec_config)\n",
      "/opt/conda/envs/marble/lib/python3.11/site-packages/numcodecs/zarr3.py:170: UserWarning: Numcodecs codecs are not in the Zarr version 3 specification and may not be supported by other zarr implementations.\n",
      "  super().__init__(**codec_config)\n",
      "/opt/conda/envs/marble/lib/python3.11/site-packages/numcodecs/zarr3.py:170: UserWarning: Numcodecs codecs are not in the Zarr version 3 specification and may not be supported by other zarr implementations.\n",
      "  super().__init__(**codec_config)\n",
      "/opt/conda/envs/marble/lib/python3.11/site-packages/numcodecs/zarr3.py:170: UserWarning: Numcodecs codecs are not in the Zarr version 3 specification and may not be supported by other zarr implementations.\n",
      "  super().__init__(**codec_config)\n",
      "/opt/conda/envs/marble/lib/python3.11/site-packages/numcodecs/zarr3.py:170: UserWarning: Numcodecs codecs are not in the Zarr version 3 specification and may not be supported by other zarr implementations.\n",
      "  super().__init__(**codec_config)\n",
      "/opt/conda/envs/marble/lib/python3.11/site-packages/numcodecs/zarr3.py:170: UserWarning: Numcodecs codecs are not in the Zarr version 3 specification and may not be supported by other zarr implementations.\n",
      "  super().__init__(**codec_config)\n",
      "/opt/conda/envs/marble/lib/python3.11/site-packages/numcodecs/zarr3.py:170: UserWarning: Numcodecs codecs are not in the Zarr version 3 specification and may not be supported by other zarr implementations.\n",
      "  super().__init__(**codec_config)\n",
      "/opt/conda/envs/marble/lib/python3.11/site-packages/numcodecs/zarr3.py:170: UserWarning: Numcodecs codecs are not in the Zarr version 3 specification and may not be supported by other zarr implementations.\n",
      "  super().__init__(**codec_config)\n",
      "/opt/conda/envs/marble/lib/python3.11/site-packages/numcodecs/zarr3.py:170: UserWarning: Numcodecs codecs are not in the Zarr version 3 specification and may not be supported by other zarr implementations.\n",
      "  super().__init__(**codec_config)\n",
      "/opt/conda/envs/marble/lib/python3.11/site-packages/numcodecs/zarr3.py:170: UserWarning: Numcodecs codecs are not in the Zarr version 3 specification and may not be supported by other zarr implementations.\n",
      "  super().__init__(**codec_config)\n",
      "/opt/conda/envs/marble/lib/python3.11/site-packages/numcodecs/zarr3.py:170: UserWarning: Numcodecs codecs are not in the Zarr version 3 specification and may not be supported by other zarr implementations.\n",
      "  super().__init__(**codec_config)\n"
     ]
    }
   ],
   "source": [
    "ds = xr.open_zarr(zarr_path)\n",
    "# readjusting the coordinates\n",
    "ds = ds.assign_coords(lon=((ds.lon + 180) % 360) - 180)\n",
    "# slicing data if necessary\n",
    "if lat_range is not None and lon_range is not None:\n",
    "    ds = ds.sel(lat=slice(*lat_range), lon=slice(*lon_range))\n",
    "\n",
    "cube = ds[channel]\n",
    "\n",
    "# split up timestamps for training the transformer and for testing\n",
    "num_t   = cube.sizes[\"t\"]\n",
    "all_idx = list(range(num_t))\n",
    "split_pt = int(0.75 * num_t)\n",
    "train_idx = all_idx[:split_pt]\n",
    "test_idx  = all_idx[split_pt:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1154659-16e9-46d8-a3c6-218db68cf7cb",
   "metadata": {},
   "source": [
    "Now, we load the GOES-16 Zarr dataset and get it ready for training. We use `xarray` to open the file, then adjust the longitude values from the `[0, 360]` range to `[-180, 180]` so that the coordinates match standard maps. If a specific latitude or longitude range is given, we crop the data to that region. After that, we select the desired channel band we want to use.\n",
    "\n",
    "Next, we split the data into two parts: 75% for training the model and 25% for testing its accuracy. This helps the model learn from most of the data while letting us evaluate how well it works on data it hasn’t seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008e2f7f-60d7-4310-8f70-5ed69d0fbff1",
   "metadata": {},
   "source": [
    "# Creating a Directory for Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261345f2-d26a-4602-90a8-d0daed91bff1",
   "metadata": {},
   "source": [
    "We want to create a directory to store all outputs related to model training, including optimizer states, training logs, and loss plots. This is important because training a model can take a long time, and we don’t want to lose progress if the process is interrupted or if we want to continue training later. \n",
    "\n",
    "This also allows us to track how the model is improving over time by saving loss values, plots, and reconstructed images at each checkpoint. In addition, by logging key metadata, it makes the training process more reproducible, helps with debugging, and allows us to compare different experiments more easily in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1222503f-04b5-4ad7-b823-c92bb392444c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/marble/lib/python3.11/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ starting from facebook/vit-mae-base\n"
     ]
    }
   ],
   "source": [
    "# create a directory \n",
    "directory = \"./goes16mae\"\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "# setting up model and optimizer \n",
    "if os.path.exists(os.path.join(directory,\"pytorch_model.bin\")):\n",
    "    print(\"✅ loading checkpoint …\")\n",
    "    model = ViTMAEForPreTraining.from_pretrained(directory, ignore_mismatched_sizes=True).train()\n",
    "    opt   = AdamW(model.parameters(), lr=learning_rate)\n",
    "    opt.load_state_dict(torch.load(os.path.join(directory,\"opt.pt\")))\n",
    "else:\n",
    "    print(\"⚠️ starting from facebook/vit-mae-base\")\n",
    "    model = ViTMAEForPreTraining.from_pretrained(\"facebook/vit-mae-base\").train()\n",
    "    opt   = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# save initial configs to directory     \n",
    "model.config.mask_ratio = mask_ratio\n",
    "model.to(device)\n",
    "processor = ViTImageProcessor.from_pretrained(\"facebook/vit-mae-base\")\n",
    "model.save_pretrained(directory)\n",
    "torch.save(opt.state_dict(), os.path.join(directory, \"opt.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c34458-aa99-45bc-88e4-40614732259e",
   "metadata": {},
   "source": [
    "Here, we are setting up the training environment for our ViT-MAE. First, we create a directory and ensures that it exists to store all outputs related to training, such as model weights and optimizer states. Then, we check whether a previously saved model checkpoint `pytorch_model.bin` exists in that directory. If it does, it loads the model and optimizer from those saved states so training can continue where it left off. If no checkpoint is found, it initializes a fresh model using Hugging Face's pre-trained `vit-mae-base` weights and creates a new optimizer. \n",
    "\n",
    "After loading or initializing the model, it sets the mask ratio, a value that controls how much of the input dataset is hidden from the model during training, and moves the model to the appropriate device (CPU/GPU). Finally, it loads a processor that helps prepare the input satellite dataset in the correct format the model expects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2d95a5-c64b-4dae-b877-398309cf338e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to create subfolders within the initial directory, loss_plots and output_plots\n",
    "def create_folders(base):\n",
    "    loss   = os.path.join(base, \"loss_plots\")\n",
    "    output = os.path.join(base, \"output_plots\")\n",
    "    os.makedirs(loss, exist_ok=True)\n",
    "    os.makedirs(output, exist_ok=True)\n",
    "    return loss, output\n",
    "\n",
    "loss, output = create_folders(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81929df-6d41-449d-b0cb-7acd252b5419",
   "metadata": {},
   "source": [
    "Using the previously created directory, this function creates two subfolders called loss_plots (for saving training-loss history) and output_plots (for storing the model’s reconstructed images). By doing this, we can easily obtain our training results, making it straightforward to monitor and analyze the progress of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e87c52-bcce-4871-a5f7-5f90ed8b3c95",
   "metadata": {},
   "source": [
    "# Model Building and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eecbfb8-a470-4858-9ee3-c6296cab3be5",
   "metadata": {},
   "source": [
    "Now that our environment is set up, we can start building and training our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff41d965-a15b-4ee9-b249-720313067eef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training for one epoch\n",
    "def train_one_epoch(model, cube, processor, train_idx, opt, crop_size, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    last_inputs = None\n",
    "\n",
    "    # iterate through every index in training timestamps\n",
    "    for idx in train_idx:\n",
    "        # load and normalize frame\n",
    "        frame = cube.isel(t=idx).values.astype(np.float32)\n",
    "        frame = (frame - frame.min()) / (frame.max() - frame.min() + 1e-6)\n",
    "        # replace NaNs or infinities with 0\n",
    "        frame = np.nan_to_num(frame, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        H, W = frame.shape\n",
    "        # randomly crop the dataset to fit optimial crop_size\n",
    "        top  = random.randint(0, H - crop_size)\n",
    "        left = random.randint(0, W - crop_size)\n",
    "        patch = frame[top:top+crop_size, left:left+crop_size]\n",
    "\n",
    "        # stack the patch three times to mimick rgb and run processor\n",
    "        rgb = np.stack([patch]*3, axis=-1)\n",
    "        inputs = processor(images=rgb, return_tensors='pt').to(device)\n",
    "        # save for later checkpoint\n",
    "        last_inputs = inputs  \n",
    "\n",
    "        # forward/backward\n",
    "        opt.zero_grad()\n",
    "        loss = model(pixel_values=inputs.pixel_values).loss\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        # free memory and clear cache\n",
    "        del frame, patch, rgb, inputs, loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss = total_loss / len(train_idx)\n",
    "    return avg_loss, last_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df59b46-92a6-46ae-b6b3-4d203d33d79d",
   "metadata": {},
   "source": [
    "The `train_one_epoch` function trains one epoch, iterating through every timestamp in the training set, and returns the average loss and last batch of processed input data used during the epoch. It sets the model to training mode and initializing variables to track total loss and store the last input batch. \n",
    "\n",
    "For each time index in the training set, it extracts the corresponding image frame, normalizes its values to the [0, 1] range, and replaces any invalid entries (NaNs or infinities) with zero to ensure the data is numerically stable. A random crop of the specified size is taken from the frame to increase input variety and help regularize the model. Since the model expects three-channel input, the single-channel image is stacked three times to mimic an RGB image.\n",
    "\n",
    "The image patch is then passed through a processor that formats it into PyTorch tensors compatible with the model’s input structure. Then, gradients are cleared, loss is computed from the forward pass, gradients are backpropagated, and the model weights are updated. The loss for each sample is accumulated to compute the average loss over the entire epoch. \n",
    "\n",
    "The function concludes by clearing memory and returning both the average training loss and the last set of processed inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5212b8be-ea49-4098-84fd-0b5e858a7f33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot reconstruction at checkpoint\n",
    "def plot_reconstruction(model, last_inputs, output_dir, epoch):\n",
    "    # put the model into evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # disable gradient tracking to speed up inference and reduce memory usage\n",
    "    with torch.no_grad():\n",
    "        out = model(**last_inputs).pixel_values.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "    plt.imshow(out)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # save the figure to output folder with the epoch number as the filename\n",
    "    plt.savefig(os.path.join(output_dir, f\"recon_epoch_{epoch}.png\"),\n",
    "                bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # return model back to training mode\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13d804e-8089-4131-9296-e5bcf454903b",
   "metadata": {},
   "source": [
    "The `plot_reconstruction` visualizes and saves a reconstruction result from the masked autoencoder model at a specific training checkpoint. First, it switches the model to evaluation mode to ensure layers like dropout or batch normalization behave consistently during inference. Then, it disables gradient tracking with `torch.no_grad()` to reduce memory usage and speed up computation, since no backpropagation is needed for this forward pass.\n",
    "\n",
    "The model generates an output reconstruction from the `last_inputs`, which is a batch of image tensors. The output is squeezed to remove the batch dimension, its axes are permuted to match the height × width × channels format expected by `matplotlib`. The reconstructed image is then plotted and saved to the specified directory using a filename that reflects the current epoch (e.g., `recon_epoch_20.png`). Finally, the model is returned to training mode so training can resume in the next epoch. This function is useful for tracking how well the model is learning to reconstruct masked inputs over time, providing a visual checkpoint alongside quantitative metrics like loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb82330-a92d-4c3b-86ab-9cd345b5b17d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save loss and model checkpoint\n",
    "def save_checkpoint(epoch, loss_history, model, opt, loss_dir, base_dir):\n",
    "    plt.figure()\n",
    "    \n",
    "    # plot loss values against epoch numbers\n",
    "    plt.plot(range(1, len(loss_history) + 1), loss_history)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Loss up to epoch {epoch}')\n",
    "    \n",
    "    # save the loss plot to the loss folder\n",
    "    plt.savefig(os.path.join(loss_dir, f'loss_epoch_{epoch}.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # retrieve the most recent loss value\n",
    "    latest_loss = loss_history[-1]\n",
    "    \n",
    "    # save the latest model's config for metadata\n",
    "    model.config.last_training_loss    = latest_loss\n",
    "    model.config.last_checkpoint_epoch = epoch\n",
    "    model.save_pretrained(base_dir)\n",
    "    torch.save(opt.state_dict(), os.path.join(base_dir, 'opt.pt'))\n",
    "    print(f\"— checkpointed at epoch {epoch}, loss={latest_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b4cbd7-69c4-47be-824e-fdd446b038b4",
   "metadata": {},
   "source": [
    "The `save_checkpoint` function manages the periodic saving of both model progress and training diagnostics during the training process. Its purpose is to preserve the state of the model and optimizer, along with visual documentation of how the training loss evolves over time.\n",
    "\n",
    "First, it creates a plot of the loss values stored in `loss_history`, mapping them against epoch numbers to provide a clear visualization of the model’s learning trajectory. This plot is saved in the designated `loss_plots` folder with the epoch number embedded in the filename, helping track training performance over time.\n",
    "\n",
    "Next, it extracts the most recent loss value to annotate the model configuration. It then updates the model’s configuration file by recording the last observed training loss and the epoch number at which the checkpoint was saved. This metadata can be useful for future analysis or resuming training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6038e2a4-c9ac-4358-b398-961628e35eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "# main training loop\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    avg_loss, last_inputs = train_one_epoch(\n",
    "        model, cube, processor, train_idx, opt, crop_size, device\n",
    "    )\n",
    "    \n",
    "    # record the epoch's average loss\n",
    "    loss_history.append(avg_loss)\n",
    "    print(f\"Epoch {epoch}/{num_epochs}  loss={avg_loss:.4f}\")\n",
    "    \n",
    "    # for every checkpoint epoch, visualize a reconstruction and save the loss in the corresponding folders\n",
    "    if epoch % checkpoint == 0:\n",
    "        plot_reconstruction(model, last_inputs, output, epoch)\n",
    "        save_checkpoint(epoch, loss_history, model, opt, loss, directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086047e8-1642-4995-824d-d68101bd7746",
   "metadata": {},
   "source": [
    "This is the main training loop that orchestrates the full training process for our model over multiple epochs. It begins by initializing an empty list, `loss_history`, which will store the average loss value for each epoch. The loop then runs from epoch 1 to the specified `num_epochs`, calling the `train_one_epoch` function on each iteration. After each epoch, the average loss is appended to `loss_history` and printed to the console, providing real-time feedback on the model’s learning progress. \n",
    "\n",
    "Every time it reaches a `checkpoint` epoch, the code performs will call `plot_reconstruction` to generate and save a reconstructed image using the latest `last_inputs` and then `save_checkpoint` to store the current model state, optimizer state, and a loss curve image. \n",
    "\n",
    "The loop ensures that loss tracking and visual reconstruction evaluations are performed regularly, and that model state is periodically saved to enable robust experimentation and reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d57fa76-1848-442e-a41a-7ab37e3d4cc1",
   "metadata": {},
   "source": [
    "# Model Evaluation and Residual Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93709613-e736-4c94-bd42-43434566acf0",
   "metadata": {},
   "source": [
    "To evaluate model performance over time, we saved loss plots and the reconstruction visualizations at regular checkpoint intervals. The loss plots, generated and saved by the `save_checkpoint` function, chart the average training loss across epochs. These plots provide a quantitative view of how the model’s reconstruction error changes over time, helping us monitor convergence and detect potential training instabilities. A consistently decreasing loss curve indicates effective learning, while plateaus or spikes may signal overfitting or learning bottlenecks.\n",
    "\n",
    "Complementing the loss plots are the qualitative reconstruction outputs generated by `plot_reconstruction`. These visualize the model’s predictions on masked satellite image patches using the final batch from each epoch. By comparing reconstructed images across checkpoints, we can observe how the model improves in restoring spatial features and structure. These outputs act as an intuitive form of residual analysis, highlighting areas where the model performs well or struggles, including blurring, missing edges, or inconsistent textures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c691aaa5-2be8-46e6-97f1-5f3787314405",
   "metadata": {},
   "source": [
    "# Limitations and Final Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a84cf2-05df-4100-b82d-db14a700643a",
   "metadata": {},
   "source": [
    "One limitation of the current training setup is that it only uses random spatial crops from individual frames as input, which may limit the model’s understanding of global spatial context. The training loop also does not include explicit validation loss tracking or early stopping, which would be valuable for detecting overfitting. Moreover, due to memory constraints, only a small number of frames are processed per epoch, and larger batch sizes or full-frame context learning are not currently feasible. \n",
    "\n",
    "Despite these limitations, the training pipeline is modular, efficient, and effective in visualizing model progression. The clear separation between training, reconstruction plotting, and checkpoint saving makes the codebase easy to extend and scale. With further tuning, this framework can serve as a strong foundation for future satellite image reconstruction or other self-supervised learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b08fc24-5dd0-469b-aa78-b8b40ae0e752",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Marble",
   "language": "python",
   "name": "marble"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
